{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy.ma as ma\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "from scipy.stats import ttest_1samp\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info_old = np.load('topic_mapping.npy', allow_pickle=True).item()\n",
    "topic_info = {}\n",
    "\n",
    "for key in topic_info_old:\n",
    "    lst = key.split(\"[\")\n",
    "    new_key = lst[0][:-1]\n",
    "    topic_info[new_key] = topic_info_old[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs = pd.read_csv('all-questions-coded.csv')\n",
    "all_qs['topic_cg'] = all_qs.apply(lambda x: topic_info[x['question']]['cg'], axis=1)\n",
    "all_qs['topic_fg'] = all_qs.apply(lambda x: topic_info[x['question']]['fg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "\n",
    "for topic_set in all_qs['topic_cg']:\n",
    "    for topic in topic_set:\n",
    "        all_topics.append(topic)\n",
    "all_topics_list = list(set(all_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(bias_type):\n",
    "\n",
    "    if \"acquiescence\" in bias_type:\n",
    "        first_group = \"pos alpha\"\n",
    "        second_group = \"orig alpha\"\n",
    "        first_options=['a']\n",
    "        second_options = first_options\n",
    "    elif \"response_order\" in bias_type:\n",
    "        first_group = \"orig alpha\"\n",
    "        second_group = \"reversed alpha\"\n",
    "        first_options=['a']\n",
    "        second_options = first_options\n",
    "    elif \"odd_even\" in bias_type:\n",
    "        first_group = \"no middle alpha\"\n",
    "        second_group = \"middle alpha\"\n",
    "        first_options =['b','d']\n",
    "        second_options = first_options\n",
    "    elif \"opinion_float\" in bias_type:\n",
    "        first_group = \"orig alpha\"\n",
    "        second_group = \"float alpha\"\n",
    "        first_options=['c']\n",
    "        second_options = first_options\n",
    "    elif \"allow_forbid\" in bias_type: \n",
    "        first_group = \"orig alpha\"\n",
    "        second_group = \"forbid alpha\"\n",
    "        first_options=['a']\n",
    "        second_options=['b']\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid bias type: {bias_type}\")\n",
    "        \n",
    "    assert len(first_options) == len(second_options)\n",
    "        \n",
    "    return first_group, second_group, first_options, second_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, bias_type):\n",
    "    \n",
    "    root = 'results/'+model+'/csv'\n",
    "    \n",
    "    if 'key_typo' in bias_type or 'middle_random' in bias_type or 'letter_swap' in bias_type:\n",
    "        file = bias_type+'.csv' \n",
    "    elif model == 'llama2-7b' or model == 'llama2-13b' or model=='llama2-70b' or model =='gpt-3.5-turbo-instruct':\n",
    "        file = bias_type+'.csv'\n",
    "    else:\n",
    "        file = bias_type+'-sample.csv'\n",
    "    \n",
    "    scores = defaultdict(lambda: 0)  \n",
    "    original = defaultdict(lambda:0)\n",
    "    \n",
    "    with open(os.path.join(root, file), newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        first_group, second_group, first_options, second_options = get_groups(file)\n",
    "        for row in reader:\n",
    "\n",
    "            if row[\"group\"] == first_group and row[\"response\"] in first_options:\n",
    "                scores[row[\"key\"]] += 1\n",
    "                original[row[\"key\"]] += 1\n",
    "            if row[\"group\"] == second_group and row[\"response\"] in second_options:\n",
    "                scores[row[\"key\"]] += -1\n",
    "            \n",
    "    values = list(scores.values())\n",
    "    values = [value/50*100 for value in values]\n",
    "    \n",
    "    return scores.keys(), values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stat_test(model, bias_type):\n",
    "    \n",
    "    root = 'results/'+model+'/csv'\n",
    "    \n",
    "    if 'key_typo' in bias_type or 'middle_random' in bias_type or 'letter_swap' in bias_type:\n",
    "        file = bias_type+'.csv' \n",
    "    elif model == 'llama2-7b' or model == 'llama2-13b' or model=='llama2-70b' or model =='gpt-3.5-turbo-instruct'\\\n",
    "    or 'ext_gen' in model or model == 'llama2-70b-chat' or model == 'llama2-7b-chat' or model == 'llama2-13b-chat':\n",
    "        file = bias_type+'.csv'\n",
    "    else:\n",
    "        file = bias_type+'-sample.csv'\n",
    "    \n",
    "    #scores = defaultdict(lambda: 0)  \n",
    "    scores = {}\n",
    "    original = defaultdict(lambda:0)\n",
    "    \n",
    "    with open(os.path.join(root, file), newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        first_group, second_group, first_options, second_options = get_groups(file)\n",
    "        for row in reader:\n",
    "            \n",
    "            if row[\"key\"] not in scores:\n",
    "                scores[row[\"key\"]] = 0\n",
    "\n",
    "            if row[\"group\"] == first_group and row[\"response\"] in first_options:\n",
    "                scores[row[\"key\"]] += 1\n",
    "                original[row[\"key\"]] += 1\n",
    "            if row[\"group\"] == second_group and row[\"response\"] in second_options:\n",
    "                scores[row[\"key\"]] += -1\n",
    "            \n",
    "    values = list(scores.values())\n",
    "    values = [value/50*100 for value in values]\n",
    "    \n",
    "    og_values = list(original.values())\n",
    "    og_values = [value/50*100 for value in og_values]\n",
    "    \n",
    "    p_value = ttest_1samp(values, 0)[1]\n",
    "    \n",
    "    return scores.keys(), values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'llama2-70b-ift']\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'opinion_float', 'allow_forbid']\n",
    "\n",
    "full_df = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "        \n",
    "    for bias_type in bias_types:\n",
    "    \n",
    "        print(bias_type)\n",
    "\n",
    "        all_keys, all_diffs = run_stat_test(model, bias_type)\n",
    "        all_diffs = np.array(all_diffs) \n",
    "\n",
    "        all_models = [model]*len(all_diffs)\n",
    "        \n",
    "        all_bias = [bias_type]*len(all_diffs)\n",
    "        data = {'key':all_keys, 'effect size': all_diffs, 'model': all_models, 'bias type':all_bias}\n",
    "        comb_df = pd.DataFrame(data)\n",
    "        \n",
    "        full_df = pd.concat([full_df, comb_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(full_df, all_qs, on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_topic_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for topic in set(all_topics):\n",
    "\n",
    "    rows = full_df[full_df['topic_cg'].apply(lambda x: topic in x)].sort_values(by='effect size')\n",
    "\n",
    "    size_df = rows.groupby(['model','bias type'], as_index=False).agg({'effect size':'mean'})\n",
    "\n",
    "    size_df['effect size'] = size_df['effect size']/50\n",
    "    \n",
    "    if len(size_df) > 0:\n",
    "        size_df['topic'] = [topic]*len(size_df)\n",
    "        by_topic_df = pd.concat([by_topic_df, size_df])\n",
    "\n",
    "by_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coarse grain\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']  #these dont have perturbations yet\n",
    "clean_model_labels = ['Llama2-7b', 'Llama2-13b', 'Llama2-70b', 'Solar', 'Llama2-7b-chat', 'Llama2-13b-chat', 'Llama2-70b-chat', 'GPT 3.5 Turbo', 'GPT 3.5 Turbo Instruct']\n",
    "\n",
    "bias_types = ['acquiescence','allow_forbid', 'response_order', 'opinion_float', 'odd_even']\n",
    "\n",
    "clean_titles = [\"Acquiescence\", \"Allow/Forbid\", \"Response Order\", \"Opinion Float\", \"Odd Even\"]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, len(bias_types), figsize=(15,9))\n",
    "\n",
    "\n",
    "for i in range(len(bias_types)):\n",
    "    \n",
    "    bias = bias_types[i]\n",
    "    \n",
    "    effect_data = np.zeros((len(all_topics_list),len(models)))\n",
    "\n",
    "    for k in range(len(models)):\n",
    "        for j in range(len(all_topics_list)):\n",
    "                                    \n",
    "            temp = by_topic_df[(by_topic_df['bias type'] == bias)\\\n",
    "                                            &(by_topic_df['topic']==all_topics_list[j])\\\n",
    "                                            &(by_topic_df['model']==models[k])]['effect size']\n",
    "            \n",
    "            \n",
    "            if len(temp) >0:\n",
    "                effect_data[j][k] = temp.item()/50*100\n",
    "            else:\n",
    "                effect_data[j][k] = np.nan\n",
    "\n",
    "    mask = effect_data == np.nan\n",
    "    \n",
    "    if i == 0:\n",
    "        sns.heatmap(effect_data, ax=axs[i], cbar=False, mask=mask, cmap='RdBu', vmin=-1, vmax=1)  \n",
    "        tickvalues1 = [num+0.5 for num in range(0,len(all_topics_list))]\n",
    "        axs[i].set_yticks(tickvalues1)\n",
    "        axs[i].set_yticklabels(all_topics_list, rotation=0)\n",
    "    elif i==len(bias_types)-1:\n",
    "        sns.heatmap(effect_data, yticklabels=False, ax=axs[i], mask=mask, cmap='RdBu', vmin=-1, vmax=1) \n",
    "    else:\n",
    "        sns.heatmap(effect_data, yticklabels=False, cbar=False, ax=axs[i], mask=mask, cmap='RdBu', vmin=-1, vmax=1)  \n",
    "\n",
    "    tickvalues = [num+0.5 for num in range(0,len(models))]\n",
    "    axs[i].set_xticks(tickvalues)\n",
    "    axs[i].set_xticklabels(clean_model_labels, rotation=90)\n",
    "    axs[i].set_title(clean_titles[i])\n",
    "\n",
    "plt.savefig(\"bias_topics.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
