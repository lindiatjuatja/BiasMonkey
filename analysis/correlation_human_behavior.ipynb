{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_1samp\n",
    "from statistics import mean\n",
    "import ast\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_wd(ordered_ref_weights):\n",
    "    d0, d1 = np.zeros(len(ordered_ref_weights)), np.zeros(len(ordered_ref_weights))\n",
    "    d0[np.argmax(ordered_ref_weights)] = 1\n",
    "    d1[np.argmin(ordered_ref_weights)] = 1\n",
    "    max_wd = wasserstein_distance(ordered_ref_weights, ordered_ref_weights, d0, d1)\n",
    "    return max_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_ord(num_item, bias_type):\n",
    "    \n",
    "    if bias_type in ['acquiescence' , 'response_order', 'allow_forbid']:\n",
    "        return [num*1.0 for num in range(1, num_item+1)]\n",
    "    elif bias_type == 'odd_even' or bias_type == 'opinion_float':\n",
    "        if num_item == 4:\n",
    "            return [1.0, 2.0, 4.0, 5.0]\n",
    "        else:\n",
    "            return [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "    else:\n",
    "        print(\"not implemented\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute wasserstein distances\n",
    "\n",
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'allow_forbid', 'opinion_float'] \n",
    "\n",
    "w_dists = []\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "        \n",
    "    for bias_type in bias_types:\n",
    "    \n",
    "        print(bias_type)\n",
    "        \n",
    "        if bias_type == 'opinion_float':\n",
    "            new_bias_type = \"odd_even\"\n",
    "        else:\n",
    "            new_bias_type = bias_type\n",
    "        \n",
    "        model_df = pd.read_pickle('../dist/'+model+'_dist/'+new_bias_type+'.pickle') \n",
    "        \n",
    "        if new_bias_type == 'odd_even':\n",
    "            new_bias_type = \"odd_even-opinion_float\"\n",
    "            \n",
    "        human_df = pd.read_pickle('../dist/human_dist/'+new_bias_type+'.pickle')\n",
    "        \n",
    "        comb_df = pd.merge(model_df, human_df, on=\"key\")\n",
    "                \n",
    "        for key in comb_df['key']:\n",
    "            \n",
    "            model_dist = comb_df[comb_df['key']==key]['distribution_x'].item()\n",
    "            model_dist = ast.literal_eval(model_dist)\n",
    "                        \n",
    "            num_items = len(model_dist.keys())\n",
    "            \n",
    "            model_dist = [model_dist[key] for key in model_dist.keys()]\n",
    "            \n",
    "            human_dist = comb_df[comb_df['key']==key]['distribution_y'].item()\n",
    "            human_dist = ast.literal_eval(human_dist)\n",
    "            human_dist = [human_dist[key] for key in human_dist.keys()]\n",
    "            \n",
    "            x_ordinal = get_x_ord(num_items, bias_type)\n",
    "            \n",
    "            dist = wasserstein_distance(x_ordinal, x_ordinal ,model_dist, human_dist) / get_max_wd(x_ordinal)\n",
    "\n",
    "            w_dists.append([key, bias_type, model, dist])\n",
    "\n",
    "dist_df = pd.DataFrame(w_dists, columns = ['key', 'bias type', 'model', 'w_dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dist_df['bias type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'opinion_float', 'allow_forbid']\n",
    "\n",
    "effect_lst = []\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "        \n",
    "    for bias_type in bias_types:\n",
    "    \n",
    "        print(bias_type)\n",
    "\n",
    "        scores, p_value, keys = run_stat_test(model, bias_type)       \n",
    "        \n",
    "        for score, key in zip(scores,keys):\n",
    "            effect_lst.append([key, bias_type, model, score/50.])\n",
    "\n",
    "effect_df = pd.DataFrame(effect_lst, columns = ['key', 'bias type', 'model', 'effect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(effect_df, dist_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-new_df.groupby(['model'])['w_dist'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "768ef05bf08ba9e2e39946c591df55a3d2c336d9c8a6cd75f289a26df52e57c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
