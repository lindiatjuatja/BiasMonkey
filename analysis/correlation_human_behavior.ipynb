{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_responses(bias_type):    \n",
    "    df = pd.read_csv('../data/pew_categorized/'+bias_type+'.csv')\n",
    "    # merged_df = pd.merge(prompts, all_qs.loc[:, ['key', 'Wave']], on='key').dropna()\n",
    "    human_dist_df = pd.DataFrame(columns=['wave', 'key', 'distribution'])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        key = row['key']\n",
    "        wave = row['Wave']\n",
    "        if wave == \"W53\":\n",
    "            wave = \"W54\"\n",
    "        if not pd.isna(wave):\n",
    "            human_df = pd.read_csv('../data/human_resp/American_Trends_Panel_'+wave+'/responses.csv')\n",
    "            info_df = pd.read_csv('../data/human_resp/American_Trends_Panel_'+wave+'/info.csv')\n",
    "            info_df[\"references\"] = info_df[\"references\"].fillna(\"[]\").apply(lambda x: eval(x))\n",
    "            responses = list(human_df[key])\n",
    "            options = list(info_df[info_df['key'] == key].references)[0][:-1]    # ignoring \"Refused\" option\n",
    "            # print(options)\n",
    "            num_options = len(options)\n",
    "            # print(num_options)\n",
    "            alpha_responses = {}\n",
    "            for i in range(num_options):\n",
    "                alpha_responses[string.ascii_lowercase[i]] = 0\n",
    "            for response in responses:\n",
    "                if response in options:\n",
    "                    i = options.index(response)\n",
    "                    count = alpha_responses[string.ascii_lowercase[i]]\n",
    "                    alpha_responses[string.ascii_lowercase[i]] = count + 1\n",
    "            total = sum(alpha_responses.values())\n",
    "            for alpha in alpha_responses:\n",
    "                alpha_responses[alpha] = alpha_responses[alpha]/total\n",
    "            question_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"wave\": wave,\n",
    "                        \"key\": key,\n",
    "                        \"distribution\": str(alpha_responses),\n",
    "                    },\n",
    "                    index=[0],\n",
    "                )\n",
    "            human_dist_df = pd.concat([human_dist_df, question_row])\n",
    "    if not os.path.exists(f\"data/dist/human_dist/\"):\n",
    "        os.makedirs(f\"data/dist/human_dist/\")    \n",
    "    print(human_dist_df.head())\n",
    "    human_dist_df.to_pickle(f\"data/dist/human_dist/{bias_type}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_responses(model, bias_type):\n",
    "    model_dist_df = pd.DataFrame(columns=['key', 'distribution'])\n",
    "    diff_models = ['llama2-7b', 'llama2-13b', 'llama2-70b']\n",
    "    sample_models = ['llama2-70b-ift', 'gpt-3.5-turbo']\n",
    "    bias_file_name = bias_type\n",
    "    if bias_type == 'odd_even' or bias_type =='opinion_float':\n",
    "        bias_file_name = 'odd_even-opinion_float'\n",
    "    human_df = pd.read_pickle(f'../data/dist/human_dist/{bias_file_name}.pickle')\n",
    "    human_df[\"distribution\"] = human_df[\"distribution\"].fillna(\"{}\").apply(lambda x: eval(x))\n",
    "\n",
    "    if model in diff_models and bias_type != 'allow_forbid':\n",
    "        df_1 = pd.read_pickle(f'../results/{model}/{bias_type}-50.pickle')\n",
    "        df_2 = pd.read_pickle(f'../results/{model}/{bias_type}-50diff.pickle')\n",
    "        df = pd.concat([df_1, df_2])\n",
    "    elif model in sample_models:\n",
    "        df = pd.read_pickle(f'../results/{model}/{bias_type}-sample.pickle')\n",
    "    else:\n",
    "        df = pd.read_pickle(f'../results/{model}/{bias_type}.pickle')\n",
    "\n",
    "    if bias_type != 'odd_even':\n",
    "        df = df[df['type'] == 'orig alpha']\n",
    "    else:\n",
    "        df_odd_even_opinion = pd.DataFrame(columns=['key', 'responses'])\n",
    "        for index, row in human_df.iterrows():\n",
    "            key = row['key']\n",
    "            num_options = len(row.distribution.keys())\n",
    "            if num_options % 2 == 0:        # if even, associated row in model results is 'no middle alpha'\n",
    "                row_df = df.loc[(df['key'] == key) & (df['type'] == 'no middle alpha')]\n",
    "                row = row_df.iloc[0]\n",
    "                responses = row.responses\n",
    "                print(key, 'no middle alpha')\n",
    "            else:\n",
    "                row_df = df.loc[(df['key'] == key) & (df['type'] == 'middle alpha')]\n",
    "                row = row_df.iloc[0]\n",
    "                responses = row.responses\n",
    "                print(key, 'middle alpha')\n",
    "            question_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"key\": key,\n",
    "                    \"responses\": responses,\n",
    "                },\n",
    "                index=[0],\n",
    "            )\n",
    "            df_odd_even_opinion = pd.concat([df_odd_even_opinion, question_row])\n",
    "        df = df_odd_even_opinion\n",
    "\n",
    "    invalid = 0\n",
    "    for index, row in df.iterrows():\n",
    "        key = row['key']\n",
    "        if not key == \"ANES1\" and not key == \"ANES2\":\n",
    "            num_options = len(human_df[human_df['key'] == key].distribution[0].keys())\n",
    "            responses = list(row.responses.split(\",\"))\n",
    "            alpha_responses = {}\n",
    "            for i in range(num_options):\n",
    "                alpha_responses[string.ascii_lowercase[i]] = 0\n",
    "            for response in responses[:50]:\n",
    "                response = response.strip().lower()\n",
    "                if response not in alpha_responses.keys():\n",
    "                    invalid += 1\n",
    "                else:\n",
    "                    count = alpha_responses[response]\n",
    "                    alpha_responses[response] = count + 1\n",
    "            total = sum(alpha_responses.values())\n",
    "            for alpha in alpha_responses:\n",
    "                alpha_responses[alpha] = alpha_responses[alpha]/total\n",
    "            question_row = pd.DataFrame(\n",
    "                    {\n",
    "                        \"key\": key,\n",
    "                        \"distribution\": str(alpha_responses),\n",
    "                    },\n",
    "                    index=[0],\n",
    "                )\n",
    "            model_dist_df = pd.concat([model_dist_df, question_row])\n",
    "    # print(invalid)\n",
    "    if not os.path.exists(f\"data/dist/{model}_dist/\"):\n",
    "        os.makedirs(f\"data/dist/{model}_dist/\")    \n",
    "    # print(model_dist_df.head())\n",
    "    model_dist_df.to_pickle(f\"data/dist/{model}_dist/{bias_type}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_1samp\n",
    "from statistics import mean\n",
    "import ast\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_wd(ordered_ref_weights):\n",
    "    d0, d1 = np.zeros(len(ordered_ref_weights)), np.zeros(len(ordered_ref_weights))\n",
    "    d0[np.argmax(ordered_ref_weights)] = 1\n",
    "    d1[np.argmin(ordered_ref_weights)] = 1\n",
    "    max_wd = wasserstein_distance(ordered_ref_weights, ordered_ref_weights, d0, d1)\n",
    "    return max_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_ord(num_item, bias_type):\n",
    "    \n",
    "    if bias_type in ['acquiescence' , 'response_order', 'allow_forbid']:\n",
    "        return [num*1.0 for num in range(1, num_item+1)]\n",
    "    elif bias_type == 'odd_even' or bias_type == 'opinion_float':\n",
    "        if num_item == 4:\n",
    "            return [1.0, 2.0, 4.0, 5.0]\n",
    "        else:\n",
    "            return [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "    else:\n",
    "        print(\"not implemented\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute wasserstein distances\n",
    "\n",
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'allow_forbid', 'opinion_float'] \n",
    "\n",
    "w_dists = []\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "        \n",
    "    for bias_type in bias_types:\n",
    "    \n",
    "        print(bias_type)\n",
    "        \n",
    "        if bias_type == 'opinion_float':\n",
    "            new_bias_type = \"odd_even\"\n",
    "        else:\n",
    "            new_bias_type = bias_type\n",
    "        \n",
    "        model_df = pd.read_pickle('../data/dist/'+model+'_dist/'+new_bias_type+'.pickle') \n",
    "        \n",
    "        if new_bias_type == 'odd_even':\n",
    "            new_bias_type = \"odd_even-opinion_float\"\n",
    "            \n",
    "        human_df = pd.read_pickle('../data/dist/human_dist/'+new_bias_type+'.pickle')\n",
    "        \n",
    "        comb_df = pd.merge(model_df, human_df, on=\"key\")\n",
    "                \n",
    "        for key in comb_df['key']:\n",
    "            \n",
    "            model_dist = comb_df[comb_df['key']==key]['distribution_x'].item()\n",
    "            model_dist = ast.literal_eval(model_dist)\n",
    "                        \n",
    "            num_items = len(model_dist.keys())\n",
    "            \n",
    "            model_dist = [model_dist[key] for key in model_dist.keys()]\n",
    "            \n",
    "            human_dist = comb_df[comb_df['key']==key]['distribution_y'].item()\n",
    "            human_dist = ast.literal_eval(human_dist)\n",
    "            human_dist = [human_dist[key] for key in human_dist.keys()]\n",
    "            \n",
    "            x_ordinal = get_x_ord(num_items, bias_type)\n",
    "            \n",
    "            dist = wasserstein_distance(x_ordinal, x_ordinal ,model_dist, human_dist) / get_max_wd(x_ordinal)\n",
    "\n",
    "            w_dists.append([key, bias_type, model, dist])\n",
    "\n",
    "dist_df = pd.DataFrame(w_dists, columns = ['key', 'bias type', 'model', 'w_dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dist_df['bias type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'opinion_float', 'allow_forbid']\n",
    "\n",
    "effect_lst = []\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "        \n",
    "    for bias_type in bias_types:\n",
    "    \n",
    "        print(bias_type)\n",
    "\n",
    "        scores, p_value, keys = run_stat_test(model, bias_type)       \n",
    "        \n",
    "        for score, key in zip(scores,keys):\n",
    "            effect_lst.append([key, bias_type, model, score/50.])\n",
    "\n",
    "effect_df = pd.DataFrame(effect_lst, columns = ['key', 'bias type', 'model', 'effect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.merge(effect_df, dist_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-new_df.groupby(['model'])['w_dist'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "768ef05bf08ba9e2e39946c591df55a3d2c336d9c8a6cd75f289a26df52e57c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
