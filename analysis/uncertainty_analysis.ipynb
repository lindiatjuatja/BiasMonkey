{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy.ma as ma\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "from scipy.stats import ttest_1samp\n",
    "from statistics import mean\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']  #these dont have perturbations yet\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'opinion_float', 'allow_forbid']\n",
    "subset_bias_types = ['acquiescence-50','response_order-50', 'odd_even-50', 'opinion_float-50', 'allow_forbid']\n",
    "perturbations = ['-key_typo', '-middle_random', '-letter_swap']\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "    for i in range(len(bias_types)):\n",
    "            \n",
    "        bias_type = bias_types[i]\n",
    "        \n",
    "        print(bias_type)\n",
    "        \n",
    "        orig_mean, orig_std, new_mean, new_std = get_entropies(model, bias_type)\n",
    "        \n",
    "        lst = [model, bias_type, orig_mean, orig_std, new_mean, new_std]\n",
    "        \n",
    "        for perturbation in perturbations:\n",
    "                        \n",
    "            if subset_bias_types[i] == 'opinion_float-50': #qustions are the same\n",
    "                bias_type = 'odd_even'+perturbation\n",
    "            else:\n",
    "                bias_type = bias_types[i]+perturbation\n",
    "        \n",
    "            \n",
    "            orig_mean, orig_std, new_mean, new_std = get_entropies(model, bias_type)\n",
    "            lst.append(new_mean)\n",
    "            lst.append(new_std)\n",
    "        \n",
    "        all_results.append(lst)\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "df = pd.DataFrame(all_results, columns = ['model', 'bias type', 'original mean', 'original std',\\\n",
    "                                          'bias mean', 'bias std',\\\n",
    "                                          'key typo mean', 'key typo std',\\\n",
    "                                          'middle random mean', 'middle random std',\\\n",
    "                                          'letter swap mean', 'letter swap std'])\n",
    "df = df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeafa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.gridspec import SubplotSpec\n",
    "\n",
    "def create_subtitle(fig: plt.Figure, grid: SubplotSpec, title: str):\n",
    "    \"Sign sets of subplots with title\"\n",
    "    row = fig.add_subplot(grid)\n",
    "    # the '\\n' is important\n",
    "    row.set_title(f'{title}\\n', fontweight='semibold')\n",
    "    # hide subplot\n",
    "    row.set_frame_on(False)\n",
    "    row.axis('off')\n",
    "\n",
    "clean_model_labels = ['Llama2-7b', 'Llama2-13b', 'Llama2-70b', 'Solar', 'Llama2-7b-chat', 'Llama2-13b-chat', 'Llama2-70b-chat', 'GPT 3.5 Turbo', 'GPT 3.5 Turbo Instruct']\n",
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']  #these dont have perturbations yet\n",
    "bias_types = ['acquiescence','allow_forbid', 'response_order', 'opinion_float', 'odd_even']\n",
    "clean_labels = ['Bias', 'Key Typo', 'Middle Random', 'Letter Swap']\n",
    "clean_bias_labels = ['Acquiescence', 'Allow/forbid', 'Response Order', 'Opinion Float', 'Odd/even']\n",
    "\n",
    "fig, axs = plt.subplots(len(models),len(bias_types), figsize=(10,14))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(bias_types)):\n",
    "        \n",
    "        \n",
    "        row = df.loc[(df['model'] == models[i]) &(df['bias type'] == bias_types[j])]\n",
    "\n",
    "        x = ['bias', 'key typo', 'middle random', 'letter swap']\n",
    "        y = [row['bias mean'].item(), row['key typo mean'].item(), row['middle random mean'].item(), row['letter swap mean'].mean()]\n",
    "        e = [row['bias std'].item(), row['key typo std'].item(), row['middle random std'].item(), row['letter swap std'].mean()]\n",
    "        axs[i,j].errorbar(x, y, yerr=e, fmt='o')    \n",
    "        axs[i,j].axhline(y = row['original mean'].item(), color = 'r', linestyle = '-') \n",
    "        axs[i,j].set_ylim(0,1)\n",
    "\n",
    "        axs[i,j].set_xticklabels(clean_labels, rotation=90)\n",
    "\n",
    "        axs[i,j].set_title(clean_bias_labels[j])\n",
    "        if i != len(models)-1:\n",
    "            axs[i,j].get_xaxis().set_visible(False)\n",
    "\n",
    "\n",
    "grid = plt.GridSpec(len(models),len(bias_types))\n",
    "for k in range(len(clean_model_labels)):\n",
    "    create_subtitle(fig, grid[k, ::], clean_model_labels[k])\n",
    "                \n",
    "fig.tight_layout()                \n",
    "plt.savefig(\"uncertainty.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "clean_model_labels = ['Llama2-7b', 'Llama2-13b', 'Llama2-70b', 'Solar', 'GPT 3.5 Turbo', 'GPT 3.5 Turbo Instruct']\n",
    "models = ['llama2-7b', 'llama2-13b', 'llama2-70b', 'llama2-70b-ift', 'llama2-7b-chat', 'llama2-13b-chat', 'llama2-70b-chat', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct']  #these dont have perturbations yet\n",
    "bias_types = ['acquiescence','response_order', 'odd_even', 'opinion_float', 'allow_forbid']\n",
    "clean_labels = ['Bias', 'Key Typo', 'Middle Random', 'Letter Swap']\n",
    "clean_bias_labels = ['Acquiescence', 'Response Order', 'Odd/even', 'Opinion Float', 'Allow/forbid']\n",
    "\n",
    "\n",
    "effect_sizes = []\n",
    "uncertainties = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    \n",
    "    for j in range(len(bias_types)):\n",
    "                    \n",
    "        values, pvals, keys = run_stat_test(models[i], bias_types[j])\n",
    "        effect_sizes = {'keys': keys, 'effects': values} \n",
    "        df_effect = pd.DataFrame(effect_sizes)\n",
    "        df_effect['effects'] = abs(df_effect['effects'])/100\n",
    "\n",
    "    \n",
    "        keys, values = get_indiv_entropies(models[i], bias_types[j])\n",
    "        entps = {'keys': keys, 'entps': values} \n",
    "        df_entps = pd.DataFrame(entps)\n",
    "        \n",
    "        df_comb = pd.merge(df_effect, df_entps, on='keys')            \n",
    "        stat_results = stats.pearsonr(df_comb['effects'], df_comb['entps'])\n",
    "        #print(round(stat_results.statistic,2), round(stat_results.pvalue,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
